{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rafeekpro/Colab/blob/main/langchain_1hr_sprint.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ii3ihksojvA"
      },
      "source": [
        "# LangChain 1-hour Sprint\n",
        "\n",
        "![LangChain Logo](https://github.com/rastringer/promptcraft_notebooks/blob/main/images/langchain.png?raw=1)\n",
        "\n",
        "LangChain is a framework for developing applications infused with LLM magic. In this notebook, we will cover some of its most useful and fun features, including:\n",
        "\n",
        "* Templates\n",
        "* Memory\n",
        "* Working with APIs\n",
        "* Chains\n",
        "* Agents\n",
        "* Vector stores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLvQkFXbTd2C"
      },
      "source": [
        "Let's start by importing some packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "A2k24lCDonBx",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0c8bf38e-5524-45d5-fd91-749711172bb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting google-cloud-aiplatform\n",
            "  Downloading google_cloud_aiplatform-1.36.4-py2.py3-none-any.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (2.11.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (1.22.3)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (3.20.3)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (23.2)\n",
            "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (2.8.0)\n",
            "Requirement already satisfied: google-cloud-bigquery<4.0.0dev,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (3.12.0)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (1.10.4)\n",
            "Requirement already satisfied: shapely<3.0.0dev in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (2.0.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.61.0)\n",
            "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.17.3)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.31.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.59.2)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.48.2)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.3.3)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.6.0)\n",
            "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.8.2)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform) (0.12.7)\n",
            "Requirement already satisfied: numpy>=1.14 in /usr/local/lib/python3.10/dist-packages (from shapely<3.0.0dev->google-cloud-aiplatform) (1.23.5)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (4.9)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2023.7.22)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (0.5.0)\n",
            "Installing collected packages: google-cloud-aiplatform\n",
            "Successfully installed google-cloud-aiplatform-1.36.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.0.344-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-experimental\n",
            "  Downloading langchain_experimental-0.0.43-py3-none-any.whl (159 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.9/159.9 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.6)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.3-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-core<0.1,>=0.0.8 (from langchain)\n",
            "  Downloading langchain_core-0.0.8-py3-none-any.whl (177 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langsmith<0.1.0,>=0.0.63 (from langchain)\n",
            "  Downloading langsmith-0.0.67-py3-none-any.whl (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.0/47.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Collecting docarray[hnswlib]<0.33.0,>=0.32.0 (from langchain)\n",
            "  Downloading docarray-0.32.1-py3-none-any.whl (215 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.3/215.3 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.1.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting orjson>=3.8.2 (from docarray[hnswlib]<0.33.0,>=0.32.0->langchain)\n",
            "  Downloading orjson-3.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: rich>=13.1.0 in /usr/local/lib/python3.10/dist-packages (from docarray[hnswlib]<0.33.0,>=0.32.0->langchain) (13.7.0)\n",
            "Collecting types-requests>=2.28.11.6 (from docarray[hnswlib]<0.33.0,>=0.32.0->langchain)\n",
            "  Downloading types_requests-2.31.0.10-py3-none-any.whl (14 kB)\n",
            "Collecting hnswlib>=0.6.2 (from docarray[hnswlib]<0.33.0,>=0.32.0->langchain)\n",
            "  Downloading hnswlib-0.7.0.tar.gz (33 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: protobuf>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from docarray[hnswlib]<0.33.0,>=0.32.0->langchain) (3.20.3)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.1.0->docarray[hnswlib]<0.33.0,>=0.32.0->langchain) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.1.0->docarray[hnswlib]<0.33.0,>=0.32.0->langchain) (2.16.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=13.1.0->docarray[hnswlib]<0.33.0,>=0.32.0->langchain) (0.1.2)\n",
            "Building wheels for collected packages: hnswlib\n",
            "  Building wheel for hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hnswlib: filename=hnswlib-0.7.0-cp310-cp310-linux_x86_64.whl size=2202684 sha256=9b72afedffce9f3f939e2de1903391617f0179612202403ad85d4370d5526159\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/ae/ec/235a682e0041fbaeee389843670581ec6c66872db856dfa9a4\n",
            "Successfully built hnswlib\n",
            "Installing collected packages: types-requests, orjson, mypy-extensions, marshmallow, jsonpointer, hnswlib, typing-inspect, langsmith, jsonpatch, langchain-core, docarray, dataclasses-json, langchain, langchain-experimental\n",
            "Successfully installed dataclasses-json-0.6.3 docarray-0.32.1 hnswlib-0.7.0 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.344 langchain-core-0.0.8 langchain-experimental-0.0.43 langsmith-0.0.67 marshmallow-3.20.1 mypy-extensions-1.0.0 orjson-3.9.10 types-requests-2.31.0.10 typing-inspect-0.9.0\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-3.17.1-py3-none-any.whl (277 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m277.6/277.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-3.17.1\n",
            "Collecting pydantic==1.10.8\n",
            "  Downloading pydantic-1.10.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic==1.10.8) (4.5.0)\n",
            "Installing collected packages: pydantic\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 1.10.13\n",
            "    Uninstalling pydantic-1.10.13:\n",
            "      Successfully uninstalled pydantic-1.10.13\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pydantic-1.10.8\n",
            "Collecting chromadb==0.3.26\n",
            "  Downloading chromadb-0.3.26-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.6/123.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=1.3 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.3.26) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.28 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.3.26) (2.31.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.3.26) (1.10.8)\n",
            "Requirement already satisfied: hnswlib>=0.7 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.3.26) (0.7.0)\n",
            "Collecting clickhouse-connect>=0.5.7 (from chromadb==0.3.26)\n",
            "  Downloading clickhouse_connect-0.6.21-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (964 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m964.2/964.2 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: duckdb>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.3.26) (0.9.2)\n",
            "Collecting fastapi>=0.85.1 (from chromadb==0.3.26)\n",
            "  Downloading fastapi-0.104.1-py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.9/92.9 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn[standard]>=0.18.3 (from chromadb==0.3.26)\n",
            "  Downloading uvicorn-0.24.0.post1-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.3.26) (1.23.5)\n",
            "Collecting posthog>=2.4.0 (from chromadb==0.3.26)\n",
            "  Downloading posthog-3.0.2-py2.py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.3.26) (4.5.0)\n",
            "Collecting pulsar-client>=3.1.0 (from chromadb==0.3.26)\n",
            "  Downloading pulsar_client-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime>=1.14.1 (from chromadb==0.3.26)\n",
            "  Downloading onnxruntime-1.16.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.3.26) (0.15.0)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.3.26) (4.66.1)\n",
            "Collecting overrides>=7.3.1 (from chromadb==0.3.26)\n",
            "  Downloading overrides-7.4.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.26) (2023.7.22)\n",
            "Requirement already satisfied: urllib3>=1.26 in /usr/local/lib/python3.10/dist-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.26) (2.0.7)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.26) (2023.3.post1)\n",
            "Collecting zstandard (from clickhouse-connect>=0.5.7->chromadb==0.3.26)\n",
            "  Downloading zstandard-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lz4 (from clickhouse-connect>=0.5.7->chromadb==0.3.26)\n",
            "  Downloading lz4-4.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.85.1->chromadb==0.3.26) (3.7.1)\n",
            "Collecting starlette<0.28.0,>=0.27.0 (from fastapi>=0.85.1->chromadb==0.3.26)\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions>=4.5.0 (from chromadb==0.3.26)\n",
            "  Downloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb==0.3.26)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb==0.3.26) (23.5.26)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb==0.3.26) (23.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb==0.3.26) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb==0.3.26) (1.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3->chromadb==0.3.26) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb==0.3.26) (1.16.0)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb==0.3.26)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb==0.3.26)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb==0.3.26) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb==0.3.26) (3.4)\n",
            "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb==0.3.26) (0.19.4)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.26) (8.1.7)\n",
            "Collecting h11>=0.8 (from uvicorn[standard]>=0.18.3->chromadb==0.3.26)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb==0.3.26)\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb==0.3.26)\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.26) (6.0.1)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb==0.3.26)\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m67.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb==0.3.26)\n",
            "  Downloading watchfiles-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb==0.3.26)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi>=0.85.1->chromadb==0.3.26) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi>=0.85.1->chromadb==0.3.26) (1.1.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb==0.3.26) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb==0.3.26) (2023.6.0)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb==0.3.26)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb==0.3.26) (1.3.0)\n",
            "Installing collected packages: monotonic, zstandard, websockets, uvloop, typing-extensions, python-dotenv, pulsar-client, overrides, lz4, humanfriendly, httptools, h11, backoff, watchfiles, uvicorn, starlette, posthog, coloredlogs, clickhouse-connect, onnxruntime, fastapi, chromadb\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed backoff-2.2.1 chromadb-0.3.26 clickhouse-connect-0.6.21 coloredlogs-15.0.1 fastapi-0.104.1 h11-0.14.0 httptools-0.6.1 humanfriendly-10.0 lz4-4.3.2 monotonic-1.6 onnxruntime-1.16.3 overrides-7.4.0 posthog-3.0.2 pulsar-client-3.3.0 python-dotenv-1.0.0 starlette-0.27.0 typing-extensions-4.8.0 uvicorn-0.24.0.post1 uvloop-0.19.0 watchfiles-0.21.0 websockets-12.0 zstandard-0.22.0\n",
            "Collecting typing-inspect==0.8.0\n",
            "  Downloading typing_inspect-0.8.0-py3-none-any.whl (8.7 kB)\n",
            "Collecting typing_extensions==4.5.0\n",
            "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect==0.8.0) (1.0.0)\n",
            "Installing collected packages: typing_extensions, typing-inspect\n",
            "  Attempting uninstall: typing_extensions\n",
            "    Found existing installation: typing_extensions 4.8.0\n",
            "    Uninstalling typing_extensions-4.8.0:\n",
            "      Successfully uninstalled typing_extensions-4.8.0\n",
            "  Attempting uninstall: typing-inspect\n",
            "    Found existing installation: typing-inspect 0.9.0\n",
            "    Uninstalling typing-inspect-0.9.0:\n",
            "      Successfully uninstalled typing-inspect-0.9.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "fastapi 0.104.1 requires typing-extensions>=4.8.0, but you have typing-extensions 4.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed typing-inspect-0.8.0 typing_extensions-4.5.0\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.35.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.16.0+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (3.8.1)\n",
            "Collecting sentencepiece (from sentence-transformers)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.19.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=51c4647a055e17b171c7d937feaa640d9bd7cfdbba1507b3f9febb59d6bcab97\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: sentencepiece, sentence-transformers\n",
            "Successfully installed sentence-transformers-2.2.2 sentencepiece-0.1.99\n"
          ]
        }
      ],
      "source": [
        "! pip install --upgrade google-cloud-aiplatform\n",
        "# LangChain\n",
        "! pip install langchain langchain-experimental langchain[docarray]\n",
        "! pip install pypdf\n",
        "! pip install pydantic==1.10.8\n",
        "# Open source vector store\n",
        "! pip install chromadb==0.3.26\n",
        "! pip install typing-inspect==0.8.0 typing_extensions==4.5.0\n",
        "# For dense vector representations of text\n",
        "! pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OPxAFOghonE3",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71a0e3c4-8dc9-4cfd-fc40-93ba39cdc624"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# Automatically restart kernel after installs so that your environment can access the new packages\n",
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGVFxl9fjt0I",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import vertexai\n",
        "\n",
        "vertexai.init()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab authentication.\n",
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "    print('Authenticated')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ta48bVqbnJJa",
        "outputId": "cf907d33-ca5c-4008-a1fc-58561e819479"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authenticated\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "PROJECT_ID = \"gncujlg-agbg-iberia-gen-ai\"  # @param {type:\"string\"}\n",
        "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
        "# Code examples may misbehave if the model is changed.\n",
        "MODEL_NAME = \"text-bison@001\""
      ],
      "metadata": {
        "id": "Mnb4Ng6lnOmD"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "N7Y7McwEz8Zk",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00a604ed-dee9-4a89-e39e-1cf66f06858f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LangChain version: 0.0.344\n",
            "Vertex AI SDK version: 1.36.4\n"
          ]
        }
      ],
      "source": [
        "# Utils\n",
        "import time\n",
        "from typing import List\n",
        "\n",
        "# Langchain\n",
        "import langchain\n",
        "from pydantic import BaseModel\n",
        "\n",
        "print(f\"LangChain version: {langchain.__version__}\")\n",
        "\n",
        "# Vertex AI\n",
        "from google.cloud import aiplatform\n",
        "from langchain.chat_models import ChatVertexAI\n",
        "from langchain.embeddings import VertexAIEmbeddings\n",
        "from langchain.llms import VertexAI\n",
        "from langchain.schema import HumanMessage, SystemMessage\n",
        "\n",
        "print(f\"Vertex AI SDK version: {aiplatform.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vertexai.init(project=PROJECT_ID,\n",
        "              location=LOCATION)"
      ],
      "metadata": {
        "id": "CcVi4wL2pXXp"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "LogO0tJ9z-73",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# We will use chat for some tasks\n",
        "chat = ChatVertexAI(\n",
        "    max_output_tokens=1024,\n",
        "    temperature=0.2,\n",
        "    top_p=0.8,\n",
        "    top_k=40,\n",
        "    verbose=True)\n",
        "\n",
        "# And we will use the general text llm for others\n",
        "llm = VertexAI(\n",
        "    max_output_tokens=1024,\n",
        "    temperature=0.2,\n",
        "    top_p=0.8,\n",
        "    top_k=40,\n",
        "    verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "by3p-b1sJl9C"
      },
      "source": [
        "The simplest LangChain use is to create chats comprising of a `SystemMessage` and `HumanMessage`. This is similar to the `context` and `user_message` that we provide the LLM using the Python client libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "lP39CLLa0F2I",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "078529d6-572d-4ecf-d444-fc133631dae0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=' Hello. How may I help you? \\n')"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "chat([HumanMessage(content=\"Hello\")])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdGjBV420Y6B",
        "tags": [],
        "outputId": "1eb4d765-352c-4248-d4e8-8ade9f12c4e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Here is a lunch recipe using kidney beans and tomatoes:\n",
            "Ingredients:\n",
            "- 1 can of kidney beans, rinsed and drained\n",
            "- 1 can of diced tomatoes, undrained\n",
            "- 1/2 cup of chopped onion\n",
            "- 1/2 cup of chopped green bell pepper\n",
            "- 1/4 cup of chopped cilantro\n",
            "- 1 tablespoon of olive oil\n",
            "- 1 teaspoon of chili powder\n",
            "- 1/2 teaspoon of ground cumin\n",
            "- 1/4 teaspoon of salt\n",
            "- 1/4 teaspoon of black pepper\n",
            "Instructions:\n",
            "1. Heat the olive oil in a large skillet over medium heat.\n",
            "2. Add the onion and green bell pepper and cook until softened, about 5 minutes.\n",
            "3. Add the kidney beans, tomatoes, chili powder, cumin, salt, and black pepper.\n",
            "4. Bring to a boil, then reduce heat and simmer for 15 minutes, or until the beans are heated through.\n",
            "5. Stir in the cilantro and serve.\n"
          ]
        }
      ],
      "source": [
        "res = chat(\n",
        "    [\n",
        "        SystemMessage(\n",
        "            content=\"You are an expert chef that thinks of imaginative recipies when people give you ingredients.\"\n",
        "        ),\n",
        "        HumanMessage(content=\"I have some kidney beans and tomatoes, what would be an easy lunch?\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(res.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiNioRJBeCeU"
      },
      "source": [
        "## Prompt templates\n",
        "\n",
        "Templates are an abstraction that can help keep prompts modular and reusable. This can be especially important in large applications which may require long and varied prompts.\n",
        "\n",
        "Templates may include few-short examples, instructions, or context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "SSkWubxs1CGM",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# The template_string parameters sets the context for the ChatPromptTemplate\n",
        "\n",
        "template_string = \"\"\"Translate the text \\\n",
        "that is delimited by triple backticks \\\n",
        "into a style that is {style}. \\\n",
        "text: ```{text}```\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "cIlbbFVA0agh",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(template_string)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upPmDaJTLgw7"
      },
      "source": [
        "The chat prompts are envisioned as a series of messages. Notice the `.messages` and `format_messages` methods in the following cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "9Yb3dCKwLq2V",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ec9b4b7-d2b7-46ff-b236-a1f92e669c26"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['style', 'text'], template='Translate the text that is delimited by triple backticks into a style that is {style}. text: ```{text}```\\n'))]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# Print out the template\n",
        "prompt_template.messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "mUeNeCwA1BiE",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb746626-51c6-4042-b9d7-f880e5849de7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['style', 'text'], template='Translate the text that is delimited by triple backticks into a style that is {style}. text: ```{text}```\\n')"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "#Let's check just the prompt\n",
        "prompt_template.messages[0].prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "dRYnKpuG0-Zz",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7131eb8c-0267-47b7-8783-cc2e50b2082e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['style', 'text']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "# Helpful method to keep track of a template's inputs\n",
        "prompt_template.messages[0].prompt.input_variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaD1QsBiKePJ"
      },
      "source": [
        "In this simple example, we translate a customer e-mail into phonetic Glaswegian."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "9RzYUX3o1G-D",
        "tags": []
      },
      "outputs": [],
      "source": [
        "translator_style = \"\"\"A translator that writes in phonetic Glaswegian.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "VOYeAk6i1Kky",
        "tags": []
      },
      "outputs": [],
      "source": [
        "customer_email = \"\"\"\n",
        "This smashing little coffee maker is simply brilliant! \\\n",
        "I'm so pleased with how easy it is to use and how quickly it brews \\\n",
        "a cracking cup of coffee. \\\n",
        "I'm over the moon with this purchase and would highly recommend it \\\n",
        "to any other coffee lover looking for a top-notch brew every time.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "mwuKkrjL1MP8",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# The format_messages method sets up the task specified in the template\n",
        "customer_messages = prompt_template.format_messages(\n",
        "                    style=translator_style,\n",
        "                    text=customer_email)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "lTqCIvvh1PSz",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8343576-0e59-40f6-bf4a-d1c395c27ac3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " A guid wee coffee maker this is, pure belter! Ah'm ower the moon wi' it, an' A wid recommend it tae ony coffee lover lookin' fur a braw brew ilka time.\n"
          ]
        }
      ],
      "source": [
        "# Call the LLM to translate to the style of the customer message\n",
        "customer_response = chat(customer_messages)\n",
        "print(customer_response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TE_nyfIgmKO6"
      },
      "source": [
        "## Parsing outputs\n",
        "\n",
        "LangChain makes it easy to return objects from the LLM in a format which we can use for further tasks (for example, adding an item of interest to a shopping cart, or providing a short list back to the LLM for additional questions).\n",
        "\n",
        "Here is an example of parsing customer reviews of a three-course meal in a restaurant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "i-karEJ_mtBl",
        "tags": []
      },
      "outputs": [],
      "source": [
        "customer_review = \"\"\"\\\n",
        "The excellent barbecue cauliflower starter left \\\n",
        "a lasting impression -- gorgeous presentation and flavors, really geared the tastebuds into action. \\\n",
        "Moving on to the main course, pretty great also. \\\n",
        "Delicious and flavorful chickpea and vegetable curry. They really nailed the buttery consistency, \\\n",
        "depth and balance of the spices. \\\n",
        "The dessert was a bit bland. I opted for a vegan chocolate mousse, \\\n",
        "hoping for a decadent and indulgent finale to my meal. \\\n",
        "It was very visually appealing but was missing the smooth, velvety \\\n",
        "texture of a great mousse.\n",
        "\"\"\"\n",
        "\n",
        "review_template = \"\"\"\\\n",
        "For the input text, extract the following details: \\\n",
        "starter: How did the reviewer find the first course? \\\n",
        "Rate either Poor, Good, or Excellent. \\\n",
        "Do the same for the main course and dessert\n",
        "\n",
        "Format the output as JSON with the following keys:\n",
        "starter\n",
        "main_course\n",
        "dessert\n",
        "\n",
        "text: {text}\n",
        "\"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Cp1dMFxkYd5H",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82886d54-a034-42b5-bed2-6c8f031b46ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_variables=['text'] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], template='For the input text, extract the following details: starter: How did the reviewer find the first course? Rate either Poor, Good, or Excellent. Do the same for the main course and dessert\\n\\nFormat the output as JSON with the following keys:\\nstarter\\nmain_course\\ndessert\\n\\ntext: {text}\\n'))]\n"
          ]
        }
      ],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(review_template)\n",
        "print(prompt_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "jHoTT2hrXzRj",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "261c99a1-c9c9-44e4-9aae-b83f8272864d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ```JSON\n",
            "{\n",
            "  \"starter\": \"Excellent\",\n",
            "  \"main_course\": \"Good\",\n",
            "  \"dessert\": \"Poor\"\n",
            "}\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "messages = prompt_template.format_messages(text=customer_review)\n",
        "response = chat(messages, temperature=0.1)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bV8VHzteZsfG"
      },
      "source": [
        "Though it looks like JSON, our output is actually a string type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "fT7RLVX9ZYeg",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d3324c6-e98c-4965-e0fb-c1dd411f56a6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "type(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jr6xxJ2ZxQr"
      },
      "source": [
        "This means we are unable to access values in this fashion (will result in an error):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "16sEr4XhZjHS",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "outputId": "16f327f1-a5db-43d8-aed4-3cf7e57cfa33"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-4116506577ac>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"main_course\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'get'"
          ]
        }
      ],
      "source": [
        "response.content.get(\"main_course\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YwKaaROZ1xa"
      },
      "source": [
        "This is where LangChain's parser comes in. Here, we import the `ResponseSchema` and `StructuredOutputParser`, which we use to define the format of the results from the LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "u1IPczDvZrJp",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from langchain.output_parsers import ResponseSchema\n",
        "from langchain.output_parsers import StructuredOutputParser\n",
        "\n",
        "starter_schema = ResponseSchema(name=\"starter\", description=\"Review of the starter\")\n",
        "main_course_schema = ResponseSchema(name=\"main_course\", description=\"Review of the main course\")\n",
        "dessert_schema = ResponseSchema(name=\"dessert\", description=\"Review of the dessert\")\n",
        "\n",
        "response_schemas = [starter_schema, main_course_schema, dessert_schema]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "tfkKmhhDb5K2",
        "tags": []
      },
      "outputs": [],
      "source": [
        "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "b4ztjoM8aXF5",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05e4004d-63b3-4061-a870-96f43277df2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
            "\n",
            "```json\n",
            "{\n",
            "\t\"starter\": string  // Review of the starter\n",
            "\t\"main_course\": string  // Review of the main course\n",
            "\t\"dessert\": string  // Review of the dessert\n",
            "}\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "format_instructions = output_parser.get_format_instructions()\n",
        "print(format_instructions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpoJt9EpcfKN"
      },
      "source": [
        "Now we can update our prior review template to include the format instructions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "Qeh2oItsa5Lu",
        "tags": []
      },
      "outputs": [],
      "source": [
        "review_template_2 = \"\"\"\\\n",
        "For the input text, extract the following details: \\\n",
        "starter: How did the reviewer find the first course? \\\n",
        "Rate either Poor, Good, or Excellent. \\\n",
        "Do the same for the main course and dessert\n",
        "\n",
        "starter\n",
        "main_course\n",
        "dessert\n",
        "\n",
        "text: {text}\n",
        "\n",
        "{format_instructions}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template=review_template_2)\n",
        "\n",
        "messages = prompt.format_messages(text=customer_review,\n",
        "                                format_instructions=format_instructions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "O779zcYwYHwV",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75763798-d980-426e-ec0a-c96a485a5b7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For the input text, extract the following details: starter: How did the reviewer find the first course? Rate either Poor, Good, or Excellent. Do the same for the main course and dessert\n",
            "\n",
            "starter\n",
            "main_course\n",
            "dessert\n",
            "\n",
            "text: The excellent barbecue cauliflower starter left a lasting impression -- gorgeous presentation and flavors, really geared the tastebuds into action. Moving on to the main course, pretty great also. Delicious and flavorful chickpea and vegetable curry. They really nailed the buttery consistency, depth and balance of the spices. The dessert was a bit bland. I opted for a vegan chocolate mousse, hoping for a decadent and indulgent finale to my meal. It was very visually appealing but was missing the smooth, velvety texture of a great mousse.\n",
            "\n",
            "\n",
            "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
            "\n",
            "```json\n",
            "{\n",
            "\t\"starter\": string  // Review of the starter\n",
            "\t\"main_course\": string  // Review of the main course\n",
            "\t\"dessert\": string  // Review of the dessert\n",
            "}\n",
            "```\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(messages[0].content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "rzBUsr3WYK5z",
        "tags": []
      },
      "outputs": [],
      "source": [
        "response = chat(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4nq3mM8crLn"
      },
      "source": [
        "Let's try it on the same review\n",
        "\n",
        "Our response starts as an `AIMessage`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "a4EVPQcQcx-_",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b30eb134-1ef1-4204-9ef2-bbe2895a666f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "langchain_core.messages.ai.AIMessage"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "type(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbjFhXIXCGZb"
      },
      "source": [
        "Here we parse the `AIMessage` into a Python dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UslUSpVTczoX",
        "tags": [],
        "outputId": "5ce9f9d7-671a-42c0-adcb-9fed11762860"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'starter': 'Excellent', 'main_course': 'Good', 'dessert': 'Poor'}"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output_dict = output_parser.parse(response.content)\n",
        "output_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-g9ngq2DbUJ"
      },
      "source": [
        "Thanks to LangChain's parser, we now have a Python dictionary which we can use for further tasks, for example taking part of the response and using it as an input to another function / process etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWTgsp_rdGq4",
        "tags": [],
        "outputId": "dfa7726e-32c9-4be0-838d-875109e456f4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(output_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkEz820ndLGJ",
        "tags": [],
        "outputId": "e87b0698-f705-41d0-dbb4-366c0b657cd5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Good'"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output_dict.get(\"main_course\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MlLBtQI-lCX"
      },
      "source": [
        "## API chains\n",
        "\n",
        "Another of LangChain's useful features is the ability to call external APIs within chains.\n",
        "\n",
        "In this example, we use the `open-meteo.com` API to get weather reports."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCOhnssF-tvb",
        "tags": [],
        "outputId": "2148d986-361e-4165-9adc-a53b3956e685"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new APIChain chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m https://api.open-meteo.com/v1/forecast?latitude=55.9533&longitude=-3.1883&hourly=temperature_2m,weathercode&current_weather=true&temperature_unit=celsius&windspeed_unit=kmh&precipitation_unit=mm&timeformat=iso8601\u001b[0m\n",
            "\u001b[33;1m\u001b[1;3m{\"latitude\":55.96,\"longitude\":-3.18,\"generationtime_ms\":0.0940561294555664,\"utc_offset_seconds\":0,\"timezone\":\"GMT\",\"timezone_abbreviation\":\"GMT\",\"elevation\":69.0,\"current_weather_units\":{\"time\":\"iso8601\",\"interval\":\"seconds\",\"temperature\":\"°C\",\"windspeed\":\"km/h\",\"winddirection\":\"°\",\"is_day\":\"\",\"weathercode\":\"wmo code\"},\"current_weather\":{\"time\":\"2023-12-01T13:00\",\"interval\":900,\"temperature\":-0.2,\"windspeed\":3.7,\"winddirection\":259,\"is_day\":1,\"weathercode\":0},\"hourly_units\":{\"time\":\"iso8601\",\"temperature_2m\":\"°C\",\"weathercode\":\"wmo code\"},\"hourly\":{\"time\":[\"2023-12-01T00:00\",\"2023-12-01T01:00\",\"2023-12-01T02:00\",\"2023-12-01T03:00\",\"2023-12-01T04:00\",\"2023-12-01T05:00\",\"2023-12-01T06:00\",\"2023-12-01T07:00\",\"2023-12-01T08:00\",\"2023-12-01T09:00\",\"2023-12-01T10:00\",\"2023-12-01T11:00\",\"2023-12-01T12:00\",\"2023-12-01T13:00\",\"2023-12-01T14:00\",\"2023-12-01T15:00\",\"2023-12-01T16:00\",\"2023-12-01T17:00\",\"2023-12-01T18:00\",\"2023-12-01T19:00\",\"2023-12-01T20:00\",\"2023-12-01T21:00\",\"2023-12-01T22:00\",\"2023-12-01T23:00\",\"2023-12-02T00:00\",\"2023-12-02T01:00\",\"2023-12-02T02:00\",\"2023-12-02T03:00\",\"2023-12-02T04:00\",\"2023-12-02T05:00\",\"2023-12-02T06:00\",\"2023-12-02T07:00\",\"2023-12-02T08:00\",\"2023-12-02T09:00\",\"2023-12-02T10:00\",\"2023-12-02T11:00\",\"2023-12-02T12:00\",\"2023-12-02T13:00\",\"2023-12-02T14:00\",\"2023-12-02T15:00\",\"2023-12-02T16:00\",\"2023-12-02T17:00\",\"2023-12-02T18:00\",\"2023-12-02T19:00\",\"2023-12-02T20:00\",\"2023-12-02T21:00\",\"2023-12-02T22:00\",\"2023-12-02T23:00\",\"2023-12-03T00:00\",\"2023-12-03T01:00\",\"2023-12-03T02:00\",\"2023-12-03T03:00\",\"2023-12-03T04:00\",\"2023-12-03T05:00\",\"2023-12-03T06:00\",\"2023-12-03T07:00\",\"2023-12-03T08:00\",\"2023-12-03T09:00\",\"2023-12-03T10:00\",\"2023-12-03T11:00\",\"2023-12-03T12:00\",\"2023-12-03T13:00\",\"2023-12-03T14:00\",\"2023-12-03T15:00\",\"2023-12-03T16:00\",\"2023-12-03T17:00\",\"2023-12-03T18:00\",\"2023-12-03T19:00\",\"2023-12-03T20:00\",\"2023-12-03T21:00\",\"2023-12-03T22:00\",\"2023-12-03T23:00\",\"2023-12-04T00:00\",\"2023-12-04T01:00\",\"2023-12-04T02:00\",\"2023-12-04T03:00\",\"2023-12-04T04:00\",\"2023-12-04T05:00\",\"2023-12-04T06:00\",\"2023-12-04T07:00\",\"2023-12-04T08:00\",\"2023-12-04T09:00\",\"2023-12-04T10:00\",\"2023-12-04T11:00\",\"2023-12-04T12:00\",\"2023-12-04T13:00\",\"2023-12-04T14:00\",\"2023-12-04T15:00\",\"2023-12-04T16:00\",\"2023-12-04T17:00\",\"2023-12-04T18:00\",\"2023-12-04T19:00\",\"2023-12-04T20:00\",\"2023-12-04T21:00\",\"2023-12-04T22:00\",\"2023-12-04T23:00\",\"2023-12-05T00:00\",\"2023-12-05T01:00\",\"2023-12-05T02:00\",\"2023-12-05T03:00\",\"2023-12-05T04:00\",\"2023-12-05T05:00\",\"2023-12-05T06:00\",\"2023-12-05T07:00\",\"2023-12-05T08:00\",\"2023-12-05T09:00\",\"2023-12-05T10:00\",\"2023-12-05T11:00\",\"2023-12-05T12:00\",\"2023-12-05T13:00\",\"2023-12-05T14:00\",\"2023-12-05T15:00\",\"2023-12-05T16:00\",\"2023-12-05T17:00\",\"2023-12-05T18:00\",\"2023-12-05T19:00\",\"2023-12-05T20:00\",\"2023-12-05T21:00\",\"2023-12-05T22:00\",\"2023-12-05T23:00\",\"2023-12-06T00:00\",\"2023-12-06T01:00\",\"2023-12-06T02:00\",\"2023-12-06T03:00\",\"2023-12-06T04:00\",\"2023-12-06T05:00\",\"2023-12-06T06:00\",\"2023-12-06T07:00\",\"2023-12-06T08:00\",\"2023-12-06T09:00\",\"2023-12-06T10:00\",\"2023-12-06T11:00\",\"2023-12-06T12:00\",\"2023-12-06T13:00\",\"2023-12-06T14:00\",\"2023-12-06T15:00\",\"2023-12-06T16:00\",\"2023-12-06T17:00\",\"2023-12-06T18:00\",\"2023-12-06T19:00\",\"2023-12-06T20:00\",\"2023-12-06T21:00\",\"2023-12-06T22:00\",\"2023-12-06T23:00\",\"2023-12-07T00:00\",\"2023-12-07T01:00\",\"2023-12-07T02:00\",\"2023-12-07T03:00\",\"2023-12-07T04:00\",\"2023-12-07T05:00\",\"2023-12-07T06:00\",\"2023-12-07T07:00\",\"2023-12-07T08:00\",\"2023-12-07T09:00\",\"2023-12-07T10:00\",\"2023-12-07T11:00\",\"2023-12-07T12:00\",\"2023-12-07T13:00\",\"2023-12-07T14:00\",\"2023-12-07T15:00\",\"2023-12-07T16:00\",\"2023-12-07T17:00\",\"2023-12-07T18:00\",\"2023-12-07T19:00\",\"2023-12-07T20:00\",\"2023-12-07T21:00\",\"2023-12-07T22:00\",\"2023-12-07T23:00\"],\"temperature_2m\":[-0.7,-0.8,-0.9,-1.1,-1.2,-1.4,-1.8,-1.9,-2.0,-2.2,-1.7,-0.9,-0.4,-0.2,-0.2,-0.4,-1.1,-1.7,-2.0,-2.2,-2.4,-2.6,-2.7,-2.8,-2.9,-2.9,-2.9,-2.9,-3.0,-2.9,-2.9,-2.9,-2.9,-2.7,-2.0,-1.0,-0.2,0.4,0.7,0.9,1.0,0.4,-0.6,-1.2,-1.5,-1.7,-1.8,-2.0,-2.2,-2.5,-2.6,-2.5,-2.2,-2.0,-1.9,-2.0,-2.2,-2.5,-2.6,-1.8,-1.1,-0.6,-0.3,-0.1,-0.4,-0.7,-0.8,-0.9,-0.9,-1.2,-1.4,-1.5,-1.5,-1.6,-1.8,-1.7,-1.6,-1.5,-1.3,-1.0,-0.9,-0.6,-0.1,0.2,0.6,1.0,1.4,1.7,1.8,1.8,1.7,1.4,0.9,0.5,0.3,0.1,-0.0,-0.2,-0.3,-0.4,-0.6,-0.8,-1.1,-1.4,-1.8,-1.9,-1.4,-0.6,-0.0,0.3,0.4,0.3,-0.3,-1.1,-1.7,-2.1,-2.4,-2.5,-2.6,-2.7,-2.6,-2.3,-1.9,-1.4,-0.8,-0.2,0.3,-1.9,-1.8,-1.5,-0.8,0.2,0.9,1.2,1.2,1.2,1.2,1.0,1.0,1.3,1.7,2.0,2.0,1.9,1.9,2.1,2.4,2.6,2.8,2.9,3.0,3.0,3.0,3.0,3.2,3.5,3.7,4.0,4.2,4.3,4.3,4.2,4.1,4.2,4.2,4.3,4.5,4.6],\"weathercode\":[1,1,0,0,1,1,1,1,1,1,1,0,0,0,0,1,1,45,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,3,3,3,3,3,2,2,2,1,1,1,2,2,2,1,45,45,45,45,45,2,2,2,2,1,1,2,2,3,2,2,45,45,45,45,45,45,45,45,45,45,45,45,45,45,3,3,3,3,3,85,85,85,85,85,85,85,3,3,3,3,3,3,2,2,2,2,2,2,1,1,1,0,0,0,0,0,0,0,0,0,2,2,2,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,61,61,61,2,2,2,3,3,3,61,61,61,61,61,61,3,3,3,61,61,61,61,61,61,3,3,3,3,3]}}\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "' The current temperature in Edinburgh, Scotland is -0.2 degrees Celsius.'"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.chains import APIChain\n",
        "from langchain.chains.api import open_meteo_docs\n",
        "\n",
        "llm = VertexAI(temperature=0)\n",
        "chain = APIChain.from_llm_and_api_docs(\n",
        "    llm,\n",
        "    open_meteo_docs.OPEN_METEO_DOCS,\n",
        "    verbose=True,\n",
        "    limit_to_domains=[\"https://api.open-meteo.com/\"],\n",
        ")\n",
        "chain.run(\n",
        "    \"How is the weather today in Edinburgh, Scotland, in Celsius?\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMao6bOb_vjj"
      },
      "source": [
        "### Wikipedia\n",
        "\n",
        "We can combine the Wikipedia pip package and LangChain's Wikipedia API wrapper get query results from the encyclopedia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SaOqZrSk-y24",
        "tags": [],
        "outputId": "7fdfb15b-4816-4d7b-effa-bfcbf7d452ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wikipedia in /home/jupyter/.local/lib/python3.10/site-packages (1.4.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from wikipedia) (4.12.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wikipedia) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2023.7.22)\n",
            "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->wikipedia) (2.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDASabouAEGS",
        "tags": [],
        "outputId": "c2eeab12-8cc9-47d6-d1c9-8aa12aecda68"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Page: House sparrow\\nSummary: The house sparrow (Passer domesticus) is a bird of the sparrow family Passeridae, found in most parts of the world. It is a small bird that has a typical length of 16 cm (6.3 in) and a mass of 24–39.5 g (0.85–1.39 oz). Females and young birds are coloured pale brown and grey, and males have brighter black, white, and brown markings. One of about 25 species in the genus Passer, the house sparrow is native to most of Europe, the Mediterranean Basin, and a large part of Asia. Its intentional or accidental introductions to many regions, including parts of Australasia, Africa, and the Americas, make it the most widely distributed wild bird.\\nThe house sparrow is strongly associated with human habitation, and can live in urban or rural settings. Though found in widely varied habitats and climates, it typically avoids extensive woodlands, grasslands, polar regions, and hot, dry deserts far away from human development. For sustenance, the house sparrow routinely feeds at home and public bird feeding stations, but naturally feeds on the seeds of grains, flowering plants and weeds. However, it is an opportunistic, omnivorous eater, and commonly catches insects, their larvae, caterpillars, invertebrates and many other natural foods.\\nBecause of its numbers, ubiquity, and association with human settlements, the house sparrow is culturally prominent. It is extensively, and usually unsuccessfully, persecuted as an agricultural pest. It has also often been kept as a pet, as well as being a food item and a symbol of lust, sexual potency, commonness, and vulgarity. Though it is widespread and abundant, its numbers have declined in some areas. The animal\\'s conservation status is listed as least concern on the IUCN Red List.\\n\\nPage: Spanish sparrow\\nSummary: The Spanish sparrow or willow sparrow (Passer hispaniolensis) is a passerine bird of the sparrow family Passeridae. It is found in the Mediterranean region and south-west and central Asia. It is very similar to the closely related house sparrow, and the two species show their close relation in a \"biological mix-up\" of hybridisation in the Mediterranean region, which complicates the taxonomy of this species.\\n\\nPage: Grasshopper sparrow\\nSummary: The grasshopper sparrow (Ammodramus savannarum) is a small New World sparrow. It belongs to the genus Ammodramus, which contains three species that inhabit grasslands and prairies. Grasshopper sparrows are sometimes found in crop fields and they will readily colonize reclaimed grassland. In the core of their range, grasshopper sparrows are dependent upon large areas of grassland where they avoid trees and shrubs. They seek out heterogenous patches of prairie that contain clumps of dead grass or other vegetation where they conceal their nest, and also contain barer ground where they forage for insects (especially grasshoppers), spiders, and seeds. Grasshopper sparrows are unusual among New World sparrows in that they sing two distinct song types, the prevalence of which varies with the nesting cycle. The primary male song, a high trill preceded by a stereotyped series of short chips, is reminiscent of the sounds of grasshoppers and is the origin of this species\\' name. Like some other birds of the central North American grasslands, this species also moves around a lot, not only via annual migrations, but individuals frequently disperse between breeding attempts or breeding seasons. Grasshopper sparrows are in steep decline across their range, even in the core of the breeding distribution in the tallgrass prairies of the central Great Plains. The Florida grasshopper sparrow (Ammodramus savannarum floridanus) is highly endangered.'"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.tools import WikipediaQueryRun\n",
        "from langchain.utilities import WikipediaAPIWrapper\n",
        "\n",
        "wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
        "\n",
        "wikipedia.run(\"To which bird family does the field sparrow belong?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2g875ERCizg"
      },
      "source": [
        "### Google search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Etx0j9SCAsAM",
        "tags": [],
        "outputId": "741369a8-4237-4eb7-92f7-c8248f3e641b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'query': 'What are the official languages in Turkmenistan, and their alphabets?',\n",
              " 'url': 'https://www.google.com/search?q=What+are+the+official+languages+in+Turkmenistan,+and+their+alphabets?',\n",
              " 'output': ' The official languages in Turkmenistan are Turkmen and Russian. Turkmen is written in a modified Latin alphabet, while Russian is written in the Cyrillic alphabet.'}"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMRequestsChain, LLMChain\n",
        "\n",
        "template = \"\"\"Between >>> and <<< are the raw search result text from google.\n",
        "Extract the answer to the question '{query}' or say \"not found\" if the information is not contained.\n",
        "Use the format\n",
        "Extracted:<answer or \"not found\">\n",
        ">>> {requests_result} <<<\n",
        "Extracted:\"\"\"\n",
        "\n",
        "PROMPT = PromptTemplate(\n",
        "    input_variables=[\"query\", \"requests_result\"],\n",
        "    template=template,\n",
        ")\n",
        "\n",
        "\n",
        "chain = LLMRequestsChain(llm_chain=LLMChain(llm=VertexAI(temperature=0), prompt=PROMPT))\n",
        "question = \"What are the official languages in Turkmenistan, and their alphabets?\"\n",
        "inputs = {\n",
        "    \"query\": question,\n",
        "    \"url\": \"https://www.google.com/search?q=\" + question.replace(\" \", \"+\"),\n",
        "}\n",
        "chain(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3ed1-PQJYso"
      },
      "source": [
        "## Memory\n",
        "\n",
        "It is essential that LLMs keep some memory of the prior interactions in a chat to better inform their answers.\n",
        "\n",
        "LangChain offers several approaches and features in this regard. For all details, see the [Memory](https://python.langchain.com/docs/modules/memory/) section of the documentation.\n",
        "\n",
        "### ConversationBufferWindowMemory\n",
        "\n",
        "Maintains a list of the interactions of the conversation over time, using the last K interactions. This can be useful for keeping a sliding window of the most recent interactions, so the buffer does not get too large"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4eQhiiYJbQV",
        "tags": [],
        "outputId": "5a06562b-1ce3-4963-b32f-b2a1902c42ca"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'history': 'Human: Hi\\nAI: How are you?\\nHuman: Fine thanks\\nAI: Great'}"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory\n",
        "\n",
        "memory = ConversationBufferWindowMemory(k=3)\n",
        "\n",
        "memory.save_context({\"input\": \"Hi\"},\n",
        "                    {\"output\": \"How are you?\"})\n",
        "memory.save_context({\"input\": \"Fine thanks\"},\n",
        "                    {\"output\": \"Great\"})\n",
        "\n",
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kN3o4bWVRK23"
      },
      "source": [
        "### ConversationTokenBufferMemory\n",
        "\n",
        "This feature instead keeps a buffer of recent interactions in memory based on token length,  rather than number of interactions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvlXfMKXRZwO",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationTokenBufferMemory\n",
        "\n",
        "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=100)\n",
        "memory.save_context({\"input\": \"All alone, she dreams of the stars!\"},\n",
        "                    {\"output\": \"As she should!\"})\n",
        "memory.save_context({\"input\": \"Baking cookies today?\"},\n",
        "                    {\"output\": \"Behold the cookies!\"})\n",
        "memory.save_context({\"input\": \"Chatbots everywhere?\"},\n",
        "                    {\"output\": \"Certainly!\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GwhfKvvRc44",
        "tags": [],
        "outputId": "1bda8be8-690f-4c09-d3aa-02d632adab13"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'history': 'Human: All alone, she dreams of the stars!\\nAI: As she should!\\nHuman: Baking cookies today?\\nAI: Behold the cookies!\\nHuman: Chatbots everywhere?\\nAI: Certainly!'}"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Nj3OPlKRled"
      },
      "source": [
        "### Conversation summaries\n",
        "\n",
        "LangChain carries forward summaries of chat messages and flushes memory after a specified number of interactions or tokens.\n",
        "\n",
        "Let's first look at using the former, `ConversationBufferWindowMemory`.\n",
        "\n",
        "We set `verbose=True` to show the prompts and information carried forward by the LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfSL0M7LPWE3",
        "tags": [],
        "outputId": "97d4382a-5e65-4265-d82f-e429d72b15f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: My favourite sport is fencing. Any tips for how I can go pro?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "' Here are some tips on how to become a professional fencer:\\n\\n1. Start young. The earlier you start fencing, the more time you will have to develop your skills and reach your full potential.\\n2. Find a good coach. A good coach can help you develop the proper technique and skills, and can also help you prepare for competitions.\\n3. Practice regularly. The more you practice, the better you will become. Try to practice at least three times per week, and make sure to focus on all aspects of fencing, including footwork, bladework, and tactics.\\n4. Compete in tournaments. Competition'"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "\n",
        "conversation_with_summary = ConversationChain(\n",
        "    llm=VertexAI(temperature=0),\n",
        "    # We set a low k=2, to only keep the last 2 interactions in memory\n",
        "    memory=ConversationBufferWindowMemory(k=2),\n",
        "    verbose=True\n",
        ")\n",
        "conversation_with_summary.predict(input=\"My favourite sport is fencing. Any tips for how I can go pro?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xo2hMT-wPmuQ",
        "tags": [],
        "outputId": "ad464d82-52bc-44c0-d75d-36228d1649ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: My favourite sport is fencing. Any tips for how I can go pro?\n",
            "AI:  Here are some tips on how to become a professional fencer:\n",
            "\n",
            "1. Start young. The earlier you start fencing, the more time you will have to develop your skills and reach your full potential.\n",
            "2. Find a good coach. A good coach can help you develop the proper technique and skills, and can also help you prepare for competitions.\n",
            "3. Practice regularly. The more you practice, the better you will become. Try to practice at least three times per week, and make sure to focus on all aspects of fencing, including footwork, bladework, and tactics.\n",
            "4. Compete in tournaments. Competition\n",
            "Human: What equipment do I need?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\" Here is a list of the equipment you will need for fencing:\\n\\n* A foil, epee, or sabre. The type of weapon you use will depend on the type of fencing you want to do.\\n* A mask. This is to protect your face from being hit by the opponent's weapon.\\n* A jacket. This is to protect your torso from being hit by the opponent's weapon.\\n* A glove. This is to protect your hand from being hit by the opponent's weapon.\\n* A pair of breeches. These are special pants that are designed for fencing.\\n* A pair of fencing\""
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation_with_summary.predict(input=\"What equipment do I need?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ih9CLZ8AQOY0",
        "tags": [],
        "outputId": "3abb18bd-7e13-4765-ea3a-267023b31e82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: My favourite sport is fencing. Any tips for how I can go pro?\n",
            "AI:  Here are some tips on how to become a professional fencer:\n",
            "\n",
            "1. Start young. The earlier you start fencing, the more time you will have to develop your skills and reach your full potential.\n",
            "2. Find a good coach. A good coach can help you develop the proper technique and skills, and can also help you prepare for competitions.\n",
            "3. Practice regularly. The more you practice, the better you will become. Try to practice at least three times per week, and make sure to focus on all aspects of fencing, including footwork, bladework, and tactics.\n",
            "4. Compete in tournaments. Competition\n",
            "Human: What equipment do I need?\n",
            "AI:  Here is a list of the equipment you will need for fencing:\n",
            "\n",
            "* A foil, epee, or sabre. The type of weapon you use will depend on the type of fencing you want to do.\n",
            "* A mask. This is to protect your face from being hit by the opponent's weapon.\n",
            "* A jacket. This is to protect your torso from being hit by the opponent's weapon.\n",
            "* A glove. This is to protect your hand from being hit by the opponent's weapon.\n",
            "* A pair of breeches. These are special pants that are designed for fencing.\n",
            "* A pair of fencing\n",
            "Human: Who are the greats of the sport I can emulate?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\" Here are some of the greatest fencers of all time:\\n\\n* Men's foil:\\n    * Stefano Cerioni (Italy)\\n    * Alexander Romankov (Russia)\\n    * Benjamin Kleibrink (Germany)\\n* Women's foil:\\n    * Valentina Vezzali (Italy)\\n    * Yelena Belova (Russia)\\n    * Nam Hyun-hee (South Korea)\\n* Men's épée:\\n    * Éric Srecki (France)\\n    * Pavel Kolobkov (Russia)\\n    * Rubén Limardo (Venezuela)\\n* Women'\""
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation_with_summary.predict(input=\"Who are the greats of the sport I can emulate?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8elpkCMvP4qI",
        "tags": [],
        "outputId": "cb9ac9ac-f35b-4c14-9018-2e01f8512dd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: What equipment do I need?\n",
            "AI:  Here is a list of the equipment you will need for fencing:\n",
            "\n",
            "* A foil, epee, or sabre. The type of weapon you use will depend on the type of fencing you want to do.\n",
            "* A mask. This is to protect your face from being hit by the opponent's weapon.\n",
            "* A jacket. This is to protect your torso from being hit by the opponent's weapon.\n",
            "* A glove. This is to protect your hand from being hit by the opponent's weapon.\n",
            "* A pair of breeches. These are special pants that are designed for fencing.\n",
            "* A pair of fencing\n",
            "Human: Who are the greats of the sport I can emulate?\n",
            "AI:  Here are some of the greatest fencers of all time:\n",
            "\n",
            "* Men's foil:\n",
            "    * Stefano Cerioni (Italy)\n",
            "    * Alexander Romankov (Russia)\n",
            "    * Benjamin Kleibrink (Germany)\n",
            "* Women's foil:\n",
            "    * Valentina Vezzali (Italy)\n",
            "    * Yelena Belova (Russia)\n",
            "    * Nam Hyun-hee (South Korea)\n",
            "* Men's épée:\n",
            "    * Éric Srecki (France)\n",
            "    * Pavel Kolobkov (Russia)\n",
            "    * Rubén Limardo (Venezuela)\n",
            "* Women'\n",
            "Human: What is my favourite sport?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "' I cannot answer that question as I do not have access to your personal information. \\n'"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Since we have now passed k=2, the LLM will be unable to answer\n",
        "conversation_with_summary.predict(input=\"What is my favourite sport?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5z_Dhb87R_6q"
      },
      "source": [
        "### ConversationSummaryBufferMemory\n",
        "\n",
        "Ensures conversational memory up to a specified token length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bp56IJU2Q6Ks",
        "tags": [],
        "outputId": "df95dd0c-17ac-4a7e-a4f2-7714dad3c483"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hi, how are you?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "' I am an AI assistant incapable of experiencing feelings in the same way humans might.  I am, however,  able to assist you with a variety of text-based activities. How may I help you today? \\n'"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.chains import ConversationChain\n",
        "\n",
        "conversation_with_summary = ConversationChain(\n",
        "    llm=llm,\n",
        "    # Change max_token_limit here after running through the conversation\n",
        "    memory=ConversationTokenBufferMemory(llm=llm, max_token_limit=600),\n",
        "    verbose=True,\n",
        ")\n",
        "conversation_with_summary.predict(input=\"Hi, how are you?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHnnO3MySR6R",
        "tags": [],
        "outputId": "9f91b760-240b-4129-9f31-e74f51587773"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi, how are you?\n",
            "AI:  I am an AI assistant incapable of experiencing feelings in the same way humans might.  I am, however,  able to assist you with a variety of text-based activities. How may I help you today? \n",
            "\n",
            "Human: I'm learning the Rust programming language\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "' Rust is an open-source systems programming language designed with safety in mind. Rust prevents many common types of bugs that plague other languages, and guarantees memory safety without garbage collection. Rust has a rich type system and an expressive macro system. It is fast and efficient, with no runtime or garbage collector. Rust is used in a wide variety of applications, including operating systems, embedded systems, and games.\\n'"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation_with_summary.predict(input=\"I'm learning the Rust programming language\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7IUBteTSKFo",
        "tags": [],
        "outputId": "7bf069e7-2898-4c81-f822-964851af1f8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi, how are you?\n",
            "AI:  I am an AI assistant incapable of experiencing feelings in the same way humans might.  I am, however,  able to assist you with a variety of text-based activities. How may I help you today? \n",
            "\n",
            "Human: I'm learning the Rust programming language\n",
            "AI:  Rust is an open-source systems programming language designed with safety in mind. Rust prevents many common types of bugs that plague other languages, and guarantees memory safety without garbage collection. Rust has a rich type system and an expressive macro system. It is fast and efficient, with no runtime or garbage collector. Rust is used in a wide variety of applications, including operating systems, embedded systems, and games.\n",
            "\n",
            "Human: What's the best book to help me?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\" There are several books that can help you learn Rust. Here are a few recommendations:\\n\\n* The Rust Programming Language: This is the official book for Rust, and it covers everything you need to know to get started with the language. It's a comprehensive book, but it's also very readable and well-organized.\\n* Rust in Action: This book is a more practical guide to Rust, with a focus on real-world examples. It covers everything from basic syntax to advanced topics like concurrency and unsafe code.\\n* Programming Rust: This book is a good choice if you're looking for a more in-depth\""
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation_with_summary.predict(input=\"What's the best book to help me?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFy53AIISZGp",
        "tags": [],
        "outputId": "96a68047-d12b-4e3b-ca38-7ba9898ed997"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi, how are you?\n",
            "AI:  I am an AI assistant incapable of experiencing feelings in the same way humans might.  I am, however,  able to assist you with a variety of text-based activities. How may I help you today? \n",
            "\n",
            "Human: I'm learning the Rust programming language\n",
            "AI:  Rust is an open-source systems programming language designed with safety in mind. Rust prevents many common types of bugs that plague other languages, and guarantees memory safety without garbage collection. Rust has a rich type system and an expressive macro system. It is fast and efficient, with no runtime or garbage collector. Rust is used in a wide variety of applications, including operating systems, embedded systems, and games.\n",
            "\n",
            "Human: What's the best book to help me?\n",
            "AI:  There are several books that can help you learn Rust. Here are a few recommendations:\n",
            "\n",
            "* The Rust Programming Language: This is the official book for Rust, and it covers everything you need to know to get started with the language. It's a comprehensive book, but it's also very readable and well-organized.\n",
            "* Rust in Action: This book is a more practical guide to Rust, with a focus on real-world examples. It covers everything from basic syntax to advanced topics like concurrency and unsafe code.\n",
            "* Programming Rust: This book is a good choice if you're looking for a more in-depth\n",
            "Human: Wish me luck!\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "' Good luck!  Rust is a powerful language that can be used to create a wide variety of applications.  I am confident that you will enjoy learning it.  If you run into any problems, there are many resources available online to help you.  The official Rust website is a great place to start, and there are also many helpful forums and communities where you can ask questions and get help.'"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Notice the buffer here is updated and clears the earlier exchanges\n",
        "# Depending on how chatty the LLM is feeling, the token limit may have\n",
        "# already been reached, and this cell will yield a generic response.\n",
        "conversation_with_summary.predict(input=\"Wish me luck!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTc4s_mTS5r0"
      },
      "source": [
        "The following cell should generate a reply that is clearly restricted to the general benefits of learning Haskell and missing the previous context of someone trying to learn Rust.\n",
        "\n",
        "Run this cell, then go back to the Keep the conversation going with summaries cell and change `max_token_limit` to 700. Then re-run the entire conversation and notice how the model relates its ouptut about learning Haskell to the context of someone trying to learn Rust."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFg9ajZgSihj",
        "tags": [],
        "outputId": "da89e525-f216-46ca-84cb-c3f2ed96f86b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi, how are you?\n",
            "AI:  I am an AI assistant incapable of experiencing feelings in the same way humans might.  I am, however,  able to assist you with a variety of text-based activities. How may I help you today? \n",
            "\n",
            "Human: I'm learning the Rust programming language\n",
            "AI:  Rust is an open-source systems programming language designed with safety in mind. Rust prevents many common types of bugs that plague other languages, and guarantees memory safety without garbage collection. Rust has a rich type system and an expressive macro system. It is fast and efficient, with no runtime or garbage collector. Rust is used in a wide variety of applications, including operating systems, embedded systems, and games.\n",
            "\n",
            "Human: What's the best book to help me?\n",
            "AI:  There are several books that can help you learn Rust. Here are a few recommendations:\n",
            "\n",
            "* The Rust Programming Language: This is the official book for Rust, and it covers everything you need to know to get started with the language. It's a comprehensive book, but it's also very readable and well-organized.\n",
            "* Rust in Action: This book is a more practical guide to Rust, with a focus on real-world examples. It covers everything from basic syntax to advanced topics like concurrency and unsafe code.\n",
            "* Programming Rust: This book is a good choice if you're looking for a more in-depth\n",
            "Human: Wish me luck!\n",
            "AI:  Good luck!  Rust is a powerful language that can be used to create a wide variety of applications.  I am confident that you will enjoy learning it.  If you run into any problems, there are many resources available online to help you.  The official Rust website is a great place to start, and there are also many helpful forums and communities where you can ask questions and get help.\n",
            "Human: Would knowing Haskell help me?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\" Knowing Haskell can definitely help you learn Rust. Haskell is a purely functional programming language, which means that it emphasizes immutability and referential transparency. This can make it easier to reason about your code and avoid bugs. Rust is also a functional language, and it borrows many concepts from Haskell. For example, Rust's pattern matching and type system are both inspired by Haskell. If you're familiar with Haskell, you'll have a head start on learning Rust.\\n\\nHere are some specific ways that knowing Haskell can help you learn Rust:\\n\\n* **Pattern matching:** Pattern matching is a powerful tool for extracting data from complex structures\""
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation_with_summary.predict(input=\"Would knowing Haskell help me?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipq1hc68ii_T"
      },
      "source": [
        "## Chains\n",
        "\n",
        "Complex applications will require chaining LLMs together, or with other components.\n",
        "\n",
        "We will cover the following types of chains:\n",
        "\n",
        "**Sequential chains**\n",
        "\n",
        "**Router chains**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUcwWDdmi0O1"
      },
      "source": [
        "### LLMChain\n",
        "\n",
        "An LLMChain simply provides a prompt to the LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rProHX1izYx",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from langchain.chains import LLMChain\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"What is the best name to describe \\\n",
        "    a company that makes {product}?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wAavEr0Ni7Pe",
        "tags": [],
        "outputId": "f3c2e907-29ec-4de5-bcf5-cbc7bd1657c3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' Assistant: Some potential names for a company that makes a saw for laminate wood include:\\n\\n- Laminate Saw Solutions\\n- The Laminate Saw Company\\n- Precision Laminate Saws\\n- Laminate Cutting Systems\\n- The Laminate Saw Experts\\n- Laminate Saw Technology\\n- Advanced Laminate Saws\\n- Laminate Saw Innovations\\n- Laminate Saw Manufacturing\\n- Laminate Saw Distribution'"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "product = \"A saw for laminate wood\"\n",
        "chain.run(product)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8792cV9FjL6C"
      },
      "source": [
        "### Sequential chain\n",
        "\n",
        "A sequential chain makes a series of calls to an LLM. It enables a pipeline-style workflow in which the output from one call becomes the input to the next.\n",
        "\n",
        "The two types include:\n",
        "\n",
        "* `SimpleSequentialChain`, where predictably each step has a single input and output, which becomes the input to the next step.\n",
        "\n",
        "* `SequentialChain`, which allows for multiple inputs and outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwKGeaiJi8uU",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from langchain.chains import SimpleSequentialChain\n",
        "from langchain.prompts import PromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25X7ALMEjPs-",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# This is an LLMChain to write a pitch for a new product\n",
        "# Let's increase the temperature to allow some imagination\n",
        "\n",
        "llm = VertexAI(temperature=0.7)\n",
        "template = \"\"\"You are an entrepreneur. Think of a ground breaking new product and write a short pitch.\n",
        "\n",
        "Title: {title}\n",
        "Entrepreneur: This is a pitch for the above product:\"\"\"\n",
        "prompt_template = PromptTemplate(input_variables=[\"title\"], template=template)\n",
        "pitch_chain = LLMChain(llm=llm, prompt=prompt_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6m7H7JavjVsL",
        "tags": []
      },
      "outputs": [],
      "source": [
        "template = \"\"\"You are a panelist on Dragon's Den. Given a \\\n",
        "description of the product, you are to explain why you think it will \\\n",
        "succeed or fail in the market.\n",
        "\n",
        "Product pitch: {pitch}\n",
        "Review by Dragon's Den panelist:\"\"\"\n",
        "prompt_template = PromptTemplate(input_variables=[\"pitch\"], template=template)\n",
        "review_chain = LLMChain(llm=llm, prompt=prompt_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RsYZq5BBjpFp",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# This is the overall chain where we run these two chains in sequence.\n",
        "overall_chain = SimpleSequentialChain(chains=[pitch_chain, review_chain], verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lI2kt13mjsAx",
        "tags": [],
        "outputId": "40e9f5bd-5f21-4a30-ed9c-797201b32bcb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
            "\u001b[36;1m\u001b[1;3m The Portable Iced Coffee Maker is the perfect solution for coffee lovers on the go. This revolutionary new product allows you to make fresh, iced coffee anywhere, anytime. With its sleek design and compact size, the Portable Iced Coffee Maker is perfect for taking with you to work, the gym, or on your next road trip. Simply add your favorite ground coffee and water, and the Portable Iced Coffee Maker will do the rest, brewing you a delicious, refreshing cup of iced coffee in just minutes. \n",
            "\n",
            "Benefits:\n",
            "\n",
            "- Makes fresh, iced coffee anywhere, anytime\n",
            "- Compact and portable design\n",
            "- Easy to use - just add ground coffee\u001b[0m\n",
            "\u001b[33;1m\u001b[1;3m While the Portable Iced Coffee Maker is a clever and innovative product, I'm not convinced that it will be a long-term success in the market.\n",
            "\n",
            "The main issue I see is that the target market for this product is relatively niche. While there are certainly many coffee lovers who would appreciate the convenience of being able to make iced coffee anywhere, anytime, I don't think the market is large enough to sustain a successful product.\n",
            "\n",
            "Additionally, the Portable Iced Coffee Maker is a relatively expensive product, which could deter some potential customers. At $50, it's more than twice the price of a traditional coffee maker.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "review = overall_chain.run(\"Portable iced coffee maker\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7LkrG1KjxRv"
      },
      "source": [
        "### Router chain\n",
        "\n",
        "A `RouterChain` dynamically selects the next chain to use for a given input.\n",
        "This feature uses the `MultiPromptChain` to select then answer with the best-suited prompt to the question.\n",
        "\n",
        "This can help a modular architecure, allowing the effective triaging of inputs between relevant prompt templates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6esOD8KCjtY0",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from langchain.chains.router import MultiPromptChain\n",
        "\n",
        "korean_template = \"\"\"\n",
        "You are an expert in korean history and culture.\n",
        "Here is a question:\n",
        "{input}\n",
        "\"\"\"\n",
        "\n",
        "spanish_template = \"\"\"\n",
        "You are an expert in spanish history and culture.\n",
        "Here is a question:\n",
        "{input}\n",
        "\"\"\"\n",
        "\n",
        "chinese_template = \"\"\"\n",
        "You are an expert in Chinese history and culture.\n",
        "Here is a question:\n",
        "{input}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqoJDRGkjzRL",
        "tags": []
      },
      "outputs": [],
      "source": [
        "prompt_infos = [\n",
        "    {\n",
        "        \"name\": \"korean\",\n",
        "        \"description\": \"Good for answering questions about Korean history and culture\",\n",
        "        \"prompt_template\": korean_template,\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"spanish\",\n",
        "        \"description\": \"Good for answering questions about Spanish history and culture\",\n",
        "        \"prompt_template\": spanish_template,\n",
        "    },\n",
        "     {\n",
        "        \"name\": \"chinese\",\n",
        "        \"description\": \"Good for answering questions about Chinese history and culture\",\n",
        "        \"prompt_template\": chinese_template,\n",
        "    },\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DalpVKISj3gl"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser\n",
        "\n",
        "llm = VertexAI(temperature=0)\n",
        "\n",
        "destination_chains = {}\n",
        "for p_info in prompt_infos:\n",
        "    name = p_info[\"name\"]\n",
        "    prompt_template = p_info[\"prompt_template\"]\n",
        "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
        "    chain = LLMChain(llm=llm, prompt=prompt)\n",
        "    destination_chains[name] = chain\n",
        "\n",
        "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
        "destinations_str = \"\\n\".join(destinations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAKrHbR-kABO"
      },
      "outputs": [],
      "source": [
        "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
        "default_chain = LLMChain(llm=llm, prompt=default_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZZ1fg0kkADv"
      },
      "outputs": [],
      "source": [
        "# Thanks to Deeplearning.ai for this template and for the\n",
        "# Langchain short course at deeplearning.ai/short-courses/.\n",
        "\n",
        "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \\\n",
        "language model select the model prompt best suited for the input. \\\n",
        "You will be given the names of the available prompts and a \\\n",
        "description of what the prompt is best suited for. \\\n",
        "You may also revise the original input if you think that revising\\\n",
        "it will ultimately lead to a better response from the language model.\n",
        "\n",
        "<< FORMATTING >>\n",
        "Return a markdown code snippet with a JSON object formatted to look like:\n",
        "```json\n",
        "{{{{\n",
        "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
        "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
        "}}}}\n",
        "```\n",
        "\n",
        "REMEMBER: \"destination\" MUST be one of the candidate prompt \\\n",
        "names specified below OR it can be \"DEFAULT\" if the input is not\\\n",
        "well suited for any of the candidate prompts.\n",
        "REMEMBER: \"next_inputs\" can just be the original input \\\n",
        "if you don't think any modifications are needed.\n",
        "\n",
        "<< CANDIDATE PROMPTS >>\n",
        "{destinations}\n",
        "\n",
        "<< INPUT >>\n",
        "{{input}}\n",
        "\n",
        "<< OUTPUT (remember to include the ```json)>>\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rlHKi0yMkAGS"
      },
      "outputs": [],
      "source": [
        "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
        "    destinations=destinations_str\n",
        ")\n",
        "router_prompt = PromptTemplate(\n",
        "    template=router_template,\n",
        "    input_variables=[\"input\"],\n",
        "    output_parser=RouterOutputParser(),\n",
        ")\n",
        "\n",
        "router_chain = LLMRouterChain.from_llm(llm, router_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-HucZwnkAII"
      },
      "outputs": [],
      "source": [
        "chain = MultiPromptChain(router_chain=router_chain,\n",
        "                         destination_chains=destination_chains,\n",
        "                         default_chain=default_chain, verbose=True\n",
        "                        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGKSiUOZkMo-"
      },
      "source": [
        "Notice in the outputs the country of speciality is prefixed eg:\n",
        "`chinese: {'input': ...`, denoting the routing to the correct expert."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6H3dVYEkHNw"
      },
      "outputs": [],
      "source": [
        "chain.run(\"What was the Han Dynasty?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMUtRs_BkRG5"
      },
      "outputs": [],
      "source": [
        "chain.run(\"What are some of typical dishes in Catalonia?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZZFI5NWkS-Z"
      },
      "outputs": [],
      "source": [
        "chain.run(\"How would I greet a friend's parents in Korean?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcF_GMsvkU91"
      },
      "outputs": [],
      "source": [
        "chain.run(\"Summarize Don Quixote in a short paragraph\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BudujtydkWUM"
      },
      "outputs": [],
      "source": [
        "# No specialist chain for carburetor advice; this\n",
        "# will be handled as any other input by the foundational model\n",
        "chain.run(\"How can I fix a carburetor?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXe_R6V_kf7G"
      },
      "source": [
        "## Agents and vectorstores\n",
        "\n",
        "This final section of the notebook will cover some of LangChain's most fun and powerful features.\n",
        "\n",
        "Agents have access to tools such as JSON, Wikipedia, Web Search, GitHub or Pandas Dataframes, and can access their capabilities depending on user input.\n",
        "\n",
        "See [here](https://python.langchain.com/docs/integrations/toolkits/) for a full list of agent toolkits.\n",
        "\n",
        "We will work with some data to perform data retrieval using the LLM with embeddings to match customer queries to products. This is known as Retrieval Augmentated Generation, or RAG.\n",
        "\n",
        "We will use the Wayfair [WANDS](https://www.aboutwayfair.com/careers/tech-blog/wayfair-releases-wands-the-largest-and-richest-publicly-available-dataset-for-e-commerce-product-search-relevance) dataset of more than 42,000 products. Here are the steps:\n",
        "\n",
        "* Download the data into a pandas dataframe and take a smaller 1,000-row sample set\n",
        "\n",
        "* Merge then generate embeddings for the product titles and descriptions\n",
        "\n",
        "* Prompt an LLM to retrieve details and relevant documents related to queries.\n",
        "\n",
        "<img src=\"https://assets.wfcdn.com/im/01139917/resize-h800-w800%5Ecompr-r85/2315/231567967/Capricornus+3+Seater+Sofa.jpg\" width=\"250\"/> <img src=\"https://assets.wfcdn.com/im/07725066/resize-h800-w800%5Ecompr-r85/1584/158440119/Vancasso+BOMOOTIUR+Stoneware+Dinnerware+-+Set+of+18.jpg\" width=\"250\"/>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCr6_IMGlUoq"
      },
      "outputs": [],
      "source": [
        "!wget -q https://raw.githubusercontent.com/wayfair/WANDS/main/dataset/product.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qY2cTO5ilY1E"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "product_df = pd.read_csv(\"product.csv\", sep='\\t')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9b6WXptlcsv"
      },
      "source": [
        "We will work with 1,000 items to avoid longer wait times for the embedding and look up processes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ndlo2oNONYxj"
      },
      "outputs": [],
      "source": [
        "product_df = product_df[:2000].dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ihFzIFAlajw"
      },
      "outputs": [],
      "source": [
        "product_df.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9w5fj9adNsUM"
      },
      "outputs": [],
      "source": [
        "len(product_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtft6ycVlqbk"
      },
      "outputs": [],
      "source": [
        "# Reduce the df to columns of interest\n",
        "product_df = product_df.filter([\"product_id\", \"product_name\", \"product_description\", \"average_rating\"], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQBJPTihlsb9"
      },
      "outputs": [],
      "source": [
        "product_df.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiHIRcVsl0F-"
      },
      "source": [
        "### Import and initialize pandas dataframe agent\n",
        "\n",
        "These tools use the `langchain-experimental` pip package installed at the start of the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "carYxcSOm_WL"
      },
      "source": [
        "### Pandas agent\n",
        "\n",
        "This agent allows us to interact with the dataframe using natural language. LangChain shows us the pandas queries it is composing to answer the questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xP8sJ5TqlzeW"
      },
      "outputs": [],
      "source": [
        "from langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent\n",
        "from langchain.agents.agent_types import AgentType\n",
        "\n",
        "agent = create_pandas_dataframe_agent(VertexAI(temperature=0), product_df, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDjIU-l8l2N_"
      },
      "outputs": [],
      "source": [
        "agent.run(\"how many rows are there?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HB3GlVLjmsgi"
      },
      "outputs": [],
      "source": [
        "agent.run(\"How many products have a rating of > 4?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8A5C_jQm4UU"
      },
      "source": [
        "### CSV agent\n",
        "\n",
        "We can also work directly on a .csv file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDxCGtBCnO4M"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame.to_csv(product_df, \"data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-BV7NdvmueF"
      },
      "outputs": [],
      "source": [
        "from langchain_experimental.agents.agent_toolkits import create_csv_agent\n",
        "\n",
        "agent = create_csv_agent(\n",
        "    VertexAI(temperature=0),\n",
        "    \"data.csv\",\n",
        "    verbose=True,\n",
        "    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YnrlNtJemx7J"
      },
      "outputs": [],
      "source": [
        "agent.run(\"How many rows are there?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8GeRRuynd2T"
      },
      "outputs": [],
      "source": [
        "agent.run(\"Do any product descriptions mention cedar wood? Output them as JSON please\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Px8rsmWWng9m"
      },
      "outputs": [],
      "source": [
        "agent.run(\"What is the square root of all ratings for product names featuring sofas\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AhOEC8snqs1"
      },
      "source": [
        "### Vector stores\n",
        "\n",
        "We will explore embeddings vectors and vector stores in more detail in [subsequent notebooks](rastringer.io.github.com/promptcraft). Let's see what's possible by concatenating our `product_title` and `product_description` columns and creating a text file from the result. We can then create embeddings and perform various retrieval and Q&A tasks.\n",
        "\n",
        "We will use the open source [Chroma](https://docs.trychroma.com/) vector store."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_afza56nyru"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.document_loaders import TextLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6OAprgzPMXu"
      },
      "source": [
        "We will embed a `text_data` column, which will be a concatenation of `product_name` and `product_description`, since both columns provide useful contextual information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDQPwlsUnyt2"
      },
      "outputs": [],
      "source": [
        "product_df['text_data'] = product_df['product_name'] + \" \" + product_df['product_description']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aookNZO4PJE0"
      },
      "outputs": [],
      "source": [
        "product_df[\"text_data\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vcXg4duWnywK"
      },
      "outputs": [],
      "source": [
        "# Save the \"text_data\" column to a text file\n",
        "text_file_path = \"combined_text_data.txt\"\n",
        "product_df['text_data'].to_csv(text_file_path, sep='\\t', index=False, header=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HgabFckIn4ML"
      },
      "outputs": [],
      "source": [
        "# load the document and split it into chunks\n",
        "loader = TextLoader(\"combined_text_data.txt\")\n",
        "documents = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwLsDcG-Ph-t"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5__alNyWPiG4"
      },
      "source": [
        "### Text splitter\n",
        "\n",
        "Splitting text is common when working with LangChain and LLMs in general. This practice means we can feed large amounts of data to LLMs for parsing or embedding in chunks, or batches.\n",
        "\n",
        "Ideally, we want to do so in a way that keeps meaningful chunks together. We will use the default recommended `RecursiveCharacterTextSplitter`. We specify a `chunk_size` and `chunk_overlap` to set an upper limit on the size and overlap between the splits / chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-w5bcOjgn5oc"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 1500,\n",
        "    chunk_overlap = 150\n",
        ")\n",
        "\n",
        "docs = text_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUGo_V-Zn6yS"
      },
      "outputs": [],
      "source": [
        "len(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBAQIkTEn8MP"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "# Clear any previous vector store\n",
        "!rm -rf ./docs/chroma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZsonVgen9rk"
      },
      "outputs": [],
      "source": [
        "# Takes ~3 mins to run\n",
        "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "db = Chroma.from_documents(docs, embedding_function)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLHE2R5kolYN"
      },
      "outputs": [],
      "source": [
        "query = \"Is there a slow cooker?\"\n",
        "docs = db.similarity_search(query, n_results=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePkNsfD_olbH"
      },
      "outputs": [],
      "source": [
        "docs[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhmN82xyn-pR"
      },
      "outputs": [],
      "source": [
        "query = \"Recommend a durable door mat\"\n",
        "docs = db.similarity_search(query, n_results=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8uYSe_eRG9K"
      },
      "outputs": [],
      "source": [
        "docs[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNN4-OPjop3Y"
      },
      "source": [
        "### Retrieval\n",
        "\n",
        "A `Retriever` is a method for answering questions based on information in an index.\n",
        "\n",
        "Here, we use `RetrievalQA` this ability with a question and answering chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhaPqMI1opLR"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "llm = VertexAI(\n",
        "    model_name=\"text-bison@001\",\n",
        "    max_output_tokens=1024,\n",
        "    temperature=0.1,\n",
        "    top_p=0.8,\n",
        "    top_k=40,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm,\n",
        "    retriever=db.as_retriever()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIhv6u72ovGo"
      },
      "source": [
        "### Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2LlRGVKopO_"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Build prompt\n",
        "template = \"\"\"Use the following pieces of context to answer the question at the end. \\\n",
        "If you don't know the answer, just say that you don't know, \\\n",
        "don't try to make up an answer. Use three sentences maximum. \\\n",
        "{context}\n",
        "Question: {question}\n",
        "Helpful Answer:\"\"\"\n",
        "QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\", \"question\"],template=template,)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpTMb0zZozCX"
      },
      "outputs": [],
      "source": [
        "# Run chain\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm,\n",
        "    retriever=db.as_retriever(),\n",
        "    return_source_documents=True,\n",
        "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2rLBHDYozE0"
      },
      "outputs": [],
      "source": [
        "question = \"Can you recommend comfortable bed sheets?\"\n",
        "result = qa_chain({\"query\": question})\n",
        "result[\"result\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5IFVCxbRNqa"
      },
      "outputs": [],
      "source": [
        "question = \"How about a Persian-style rug for my living room.\"\n",
        "result = qa_chain({\"query\": question})\n",
        "result[\"result\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHr7hwltRiMc"
      },
      "source": [
        "## Summary\n",
        "\n",
        "In this whirlwind tour of some of LangChain's features, we covered:\n",
        "\n",
        "* Memory\n",
        "* Chains\n",
        "* Agents\n",
        "* Vector stores\n",
        "\n",
        "LangChain is a fast-evolving project. To explore more features and keep up-to-date with developments, please see the [website](https://www.langchain.com/) or [Python documentation](https://python.langchain.com/docs/get_started/introduction).\n",
        "\n",
        "With thanks to Harrison Chase and the excellent LangChain courses at [deeplearning.ai](https://deeplearning.ai/short-courses)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "environment": {
      "kernel": "python3",
      "name": "tf2-cpu.2-11.m113",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-cpu.2-11:m113"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}